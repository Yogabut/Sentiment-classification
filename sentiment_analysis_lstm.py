# -*- coding: utf-8 -*-
"""Sentiment Analysis LSTM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_ITpyC2PoCaZOfFQmDPUSx4h9zUEdRwq

# **Analisis Sentimen dan Prediksi Sentimen Crypto**

# Labeling Data
"""

# Labeling data dilakukan dengan cara menggunakan fungsi percabangan yang dikondinsikan dengan kieadaan pada kalimat menurut perasaan retweet

"""# Open Data dari Google Drive"""

import numpy
import keras.models
import tensorflow

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
df= pd.read_csv('/content/drive/MyDrive/dataset/data.csv')
df

def determine_sentiment(df):
    if df['happy'] > 0.5 or (df['happy'] >= 0.5 and df['surprise'] >= 0.5) :
        return 'positive'
    elif df['sad'] > 0.5  or df['fear'] > 0.5 or df['angry'] > 0.5 or (df['sad'] >= 0.33 and df['fear'] >= 0.33) or (df['sad'] >= 0.33 and df['angry'] >= 0.33) or (df['fear'] > 0.33 and df['angry'] > 0.33) or (df['sad'] >= 0.5 and df['surprise'] >= 0.5) or (df['fear'] >= 0.5 and df['surprise'] >= 0.5) or (df['angry'] >= 0.5 and df['surprise'] >= 0.5):
        return 'negative'
    else:
        return 'neutral'

df['sentiment'] = df.apply(determine_sentiment, axis=1)
df

df = df[['tweet_text', 'sentiment']]
df

df = df.rename(columns={'tweet_text': 'Content', 'sentiment' : 'Labels'})
df

df.shape

df.info

# Menghitung jumlah data duplikat pada kolom 'Content'
jumlah_duplikasi = df['Content'].duplicated().sum()
print('Jumlah Duplikasi:', jumlah_duplikasi)

# Menampilkan data yang duplikat pada kolom 'Content'
data_duplikasi = df[df['Content'].duplicated(keep=False)]
print(data_duplikasi)

df.describe()

print("Jumlah data kosong:\n", df.isna().sum())

"""#Balancing Data"""

# Separate the data by sentiment category
negative_data = df[df["Labels"] == "negative"]
positive_data = df[df["Labels"] == "positive"]
neutral_data = df[df["Labels"] == "neutral"]

balanced_positive_data = positive_data.sample(n=1500, random_state=42, replace=True)

# Concatenate the balanced data from each sentiment category
balanced_data = pd.concat([negative_data, balanced_positive_data, neutral_data])

# Shuffle the data
balanced_data = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)

# Print the value counts to verify the balance
print(balanced_data["Labels"].value_counts())

balanced_data.to_csv('balanced_data_crypto_sentiment.csv', index=False)

df = pd.read_csv('/content/balanced_data_crypto_sentiment.csv')
df.Labels.value_counts()
df

import seaborn as sns
import matplotlib.pyplot as plt
# sns.countplot(x='Labels', data=df)
plt.figure(figsize=(8, 6))
df['Labels'].value_counts().plot(kind='bar', color=['green','blue', 'red'])
plt.title('Sentiment Distribution')
plt.xlabel('Labels')
plt.ylabel('Total')
plt.xticks(rotation=0)
plt.show()

"""# **Preprocessing**

# Preprocessing Tahap 0
"""

#Import
import tweepy
import json
import csv
import re
import string
import random
import nltk
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import json

#From
from time import time
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from collections import Counter
from matplotlib.patches import Patch
from sklearn import metrics
from sklearn.model_selection import KFold, train_test_split, cross_val_score
from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB, BernoulliNB
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.pipeline import make_pipeline
from wordcloud import WordCloud
from wordcloud import WordCloud,ImageColorGenerator

# cleansing data
df
def regex_func(dataTw):
    dataRegex = []
    for datax in range(0, len(dataTw)):
        regex = dataTw.iloc[datax, 0]
        regex = re.sub(r'http\S+', '', regex)
        regex = re.sub('@[^\s]+', '', regex)
        regex = re.sub(r'#[^\s]+', '', regex)
        regex = regex.encode("ascii", "ignore")
        regex = regex.decode()
        regex = re.sub('\s+',' ', regex)
        mark_list_word = regex.split()
        if len(mark_list_word) > 10:
            dataRegex.append(regex)
    return dataRegex

Data1_reg = regex_func(df)
Data1_reg

df.to_csv("pre0.csv", index = False)

"""# Preprocessing Tahap 1"""

import re
def removeSpecialChar(tweet):
    tweet = tweet.replace('\\t'," ").replace('\\n'," ").replace("\\u"," ").replace("\\"," ").replace("_","").replace(".","")
    tweet = tweet.encode('ascii','replace').decode('ascii')
    tweet = ' '.join(re.sub("([@#][A-Za-z0-9]+)|(\w+:\/\/\S+)"," ",tweet).split())
    tweet = tweet.replace("http://"," ").replace("http://"," ")
    return tweet

df['tweet_clean_1'] = df['Content'].apply(lambda x : removeSpecialChar(x))
df

import string
def removePunctuation(tweet):
    tweet = tweet.translate(str.maketrans("","",string.punctuation))
    return tweet

df['tweet_clean_1'] = df['tweet_clean_1'].apply(lambda x : removePunctuation(x))
df

# Remove Emoticon
def removeEmoticons(tweet):
    emoji_pattern = re.compile("["
        u"\U0001F600-\U0001F64F"  # emoticons
        u"\U0001F300-\U0001F5FF"  # symbols & pictographs
        u"\U0001F680-\U0001F6FF"  # transport & map symbols
        u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                           "]+", flags=re.UNICODE)
    return emoji_pattern.sub(r'', tweet)

df['tweet_clean_1'] = df['tweet_clean_1'].apply(lambda x : removeEmoticons(x))
df

#Remover Single Char
def removeSingleChr(tweet):
  # menggunakan regex.function()
  tweet = re.sub(r"\b[a-zA-Z]\b","",tweet)
  # /b word boundary digunakakan untuk  memfilter huruf yang hanya terdiri dari satu karakter
  # setiap huruf akan di cek apakah setelah huruf tersebut terdapat spasi atau tidak
  #\b backspace batas word , contoh \bA\b
  return tweet

df['tweet_clean_1'] = df['tweet_clean_1'].apply(lambda x : removeSingleChr(x))
df

def removeNum(tweet):
    return re.sub(r"\d+","",tweet)

df['tweet_clean_1'] =df['tweet_clean_1'].apply(lambda x : removeNum(x))
df

df.dropna(subset=['tweet_clean_1'], inplace=True)
df.reset_index(drop=True, inplace=True)

df

df.to_excel('pre1.xlsx' , index = False)

"""# Preprocesing Tahap 2"""

from nltk.tokenize import word_tokenize
from nltk.probability import FreqDist
import nltk

nltk.download("punkt")

#Melakukan Tokenize
df['tweet_tokenize_2'] = df['tweet_clean_1'].apply(lambda x : word_tokenize(x))

pd.set_option('display.max_colwidth', 180)
df.head(5)

norm = pd.read_csv('/content/drive/MyDrive/dataset/pre1.csv')

#Melakukan normalisasi Pada kata kebentuk Dasar
normalized_tweet_docx = norm

normalized_tweet_dict={}
for index , row in normalized_tweet_docx.iterrows():
    if row[0].strip() not in normalized_tweet_dict :
    #jika kata x dalam doct tweet tidak berada di dict tweet maka
        normalized_tweet_dict[row[0].strip()] = row[1].strip()
    # dict tweeet kata x pada row 0 docx tweet = row 1 pada docx tweet

def normalizedTerm(tweet):
  #mengakses key , dan mengembalikan value
  return [normalized_tweet_dict[term] if term in normalized_tweet_dict else term for term in tweet]
  #kembalikan bentuk normalisasi dari kata x jika kata x berada dalam dict tweet ,  namun jika tidak kembalikan kata x teresebut

df['normalized_stem_1'] = df['tweet_tokenize_2'].apply(lambda x : normalizedTerm(x))
df

df.head(5)

from nltk.corpus import stopwords
nltk.download('stopwords')
list_stopwords = stopwords.words('indonesian','english')

list_stopwords.extend([
    "cm", "ny", "d",'cc','luu','we''nis','la','tah',
    'amp', 'md','krn','tp','qrt',
    'bbb','mm','bb','mb','bm','bbb','mhb','bbb'
])

# convert list to dictionary
list_stopwords = set(list_stopwords)

#mengembalikan words yang tidak berada di dalam list stopwords
def cleanStopwords(words):
    return [w for w in words if w not in list_stopwords]

df['remove_stopwords'] = df['normalized_stem_1'].apply(lambda x : cleanStopwords(x))
df

df.to_csv('pre1.csv' , index=False)

"""# Preprocessing Tahap 3"""

! pip install Sastrawi
from sklearn.pipeline import Pipeline
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory

def stemming(tweet):
    factory = StemmerFactory()
    stemmer = factory.create_stemmer()
    word =[]
    for t in tweet:
        dt = stemmer.stem(t)
        word.append(dt)
    t_clean=[]
    t_clean= " ".join(word)
    print(t_clean)
    return t_clean

df['steming_2'] = df['remove_stopwords'].apply(lambda x: stemming(x))
df

df.to_csv('preprocessing.csv' , index=False)

df = pd.read_csv('preprocessing.csv')
df

df=df[['Content', 'Labels']] #content ganti ke steming_2

df.head()

df.to_csv("afterprepo.csv", index = False)

# Sortir data dan label berdasarkan sentimen
neu = df.loc[df['Labels'] == 'neutral'].Content.tolist() #content ganti ke steming_2
pos = df.loc[df['Labels'] == 'positive'].Content.tolist()
neg = df.loc[df['Labels'] == 'negative'].Content.tolist()

neu_label = df.loc[df['Labels'] == 'neutral'].Labels.tolist()
pos_label = df.loc[df['Labels'] == 'positive'].Labels.tolist()
neg_label = df.loc[df['Labels'] == 'negative'].Labels.tolist()

total_data = pos + neu + neg
labels = pos_label + neu_label + neg_label

print("Pos: %s, Neu: %s, Neg: %s" % (len(pos), len(neu), len(neg)))
print("Total data: %s" % len(total_data))

"""# Word Cloud

"""

import matplotlib.pyplot
import seaborn as sns
import wordcloud
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
import matplotlib.pyplot as plt

df['Content'] = df['Content'].astype(str)
comment_words = " ".join(cat for cat in df.Content)   #content ganti ke steming_2

wordcloud1 = WordCloud(
        width=3000,
        height=2000,
        random_state=1,
        background_color="white",
        collocations=False,
        stopwords=STOPWORDS,
        ).generate(comment_words)

wordcloud1.to_file("wordcloud.png")
plt.figure(figsize = (10,10), facecolor = None)
plt.imshow(wordcloud1)
plt.axis("off")
plt.title("Kata yang Paling Banyak Muncul", fontsize = 17)
plt.tight_layout(pad = 1)

plt.show()

"""# Feature Extraction"""

import pickle
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from collections import defaultdict

max_features = 10000
# Assuming total_data is your list containing data
total_data = [str(item) for item in total_data]

tokenizer = Tokenizer(num_words=max_features, split=' ', lower=True)
tokenizer.fit_on_texts(total_data)
with open('tokenizer.pickle', 'wb') as handle:
    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)
    print("tokenizer.pickle has created!")

X = tokenizer.texts_to_sequences(total_data)

vocab_size = len(tokenizer.word_index)
maxlen = max(len(x) for x in X)

X = pad_sequences(X)
with open('x_pad_sequences.pickle', 'wb') as handle:
    pickle.dump(X, handle, protocol=pickle.HIGHEST_PROTOCOL)
    print("x_pad_sequences.pickle has created!")

Y = pd.get_dummies(labels)
Y = Y.values

with open('y_labels.pickle', 'wb') as handle:
    pickle.dump(Y, handle, protocol=pickle.HIGHEST_PROTOCOL)
    print("y_labels.pickle has created!")

"""# Splitting Dataset"""

from sklearn.model_selection import train_test_split

file = open("x_pad_sequences.pickle",'rb')
X = pickle.load(file)
file.close()

file = open("y_labels.pickle",'rb')
Y = pickle.load(file)
file.close()

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=1)

"""#Training"""

import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, SimpleRNN, Activation, Dropout
from tensorflow.keras import optimizers
from tensorflow.keras.optimizers import RMSprop, Adam
from tensorflow.keras.callbacks import EarlyStopping, TensorBoard
from tensorflow.keras.layers import Flatten
from tensorflow.keras import backend as K

class myCallback(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs={}):
        if (logs.get('accuracy') > 0.84 and logs.get('val_accuracy') > 0.74):
            print("\n Accuracy achieved, training stopped...")
            self.model.stop_training = True

Callback = myCallback()

embed_dim = 200
units = 200

model = Sequential()
model.add(Embedding(max_features, embed_dim, input_length=X.shape[1]))
model.add(LSTM(units, dropout=0.4))
model.add(Dense(3,activation='softmax'))

adam = optimizers.Adam(learning_rate = 0.001)
model.compile(loss = 'categorical_crossentropy', optimizer = adam, metrics = ['accuracy'])
print(model.summary())
history = model.fit(X_train,
                    y_train,
                    epochs=100,
                    batch_size=32,
                    validation_data=(X_test, y_test),
                    verbose=1,
                    callbacks=[Callback])

"""#Evaluation"""

from sklearn import metrics

predictions = model.predict(X_test)
y_pred = predictions
matrix_test = metrics.classification_report(y_test.argmax(axis=1), y_pred.argmax(axis=1), zero_division=0 )
print("Testing selesai")
print(matrix_test)

"""# Cross Validation"""

import numpy as np
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
from sklearn.model_selection import KFold

kf = KFold(n_splits=5,random_state=42,shuffle=True)

accuracies = []

y = Y

embed_dim = 400
units = 400

for iteration, data in enumerate(kf.split(X), start=1):

    data_train   = X[data[0]]
    target_train = y[data[0]]

    data_test    = X[data[1]]
    target_test  = y[data[1]]


    model = Sequential()
    model.add(Embedding(max_features, embed_dim, input_length=X.shape[1]))
    model.add(LSTM(units, dropout=0.2))
    model.add(Dense(3,activation='softmax'))
    # model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])

    adam = optimizers.Adam(learning_rate = 0.001)
    model.compile(loss = 'categorical_crossentropy', optimizer = adam, metrics = ['accuracy'])

    es = EarlyStopping(monitor='val_loss', mode='min', verbose=0)
    history = model.fit(X_train, y_train, epochs=10, batch_size=10, validation_data=(X_test, y_test), verbose=0, callbacks=[es])

    predictions = model.predict(X_test)
    y_pred = predictions

    # for the current fold only
    accuracy = accuracy_score(y_test.argmax(axis=1), y_pred.argmax(axis=1))

    print("Training ke-", iteration)
    print(classification_report(y_test.argmax(axis=1), y_pred.argmax(axis=1)))
    print("======================================================")

    accuracies.append(accuracy)

# this is the average accuracy over all folds
average_accuracy = np.mean(accuracies)

print()
print()
print()
print("Rata-rata Accuracy: ", average_accuracy)

"""# Visualization"""

# Commented out IPython magic to ensure Python compatibility.
# history.history
import matplotlib.pyplot as plt
plt.style.use('ggplot')

def plot_history(history):
    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']
    loss = history.history['loss']
    val_loss = history.history['val_loss']
    x = range(1, len(acc) + 1)

    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(x, acc, 'b', label='Training acc')
    plt.plot(x, val_acc, 'r', label='Validation acc')
    plt.title('Training and validation accuracy')
    plt.legend()
    plt.subplot(1, 2, 2)
    plt.plot(x, loss, 'b', label='Training loss')
    plt.plot(x, val_loss, 'r', label='Validation loss')
    plt.title('Training and validation loss')
    plt.legend()

# %matplotlib inline
plot_history(history)

model.save('model.h5')
print("Model has created!")

"""# Predict"""

import re
from keras.models import load_model

input_text = """
bitcoin still up
"""

def cleansing(sent):
    string = sent.lower()
    string = re.sub(r'[^a-zA-Z0-9]|([^0-9A-Za-z \t])|(\w+:\/\/\S+)|^rt|http.+?', ' ', string)
    string = re.sub(r"\d+", "", string)
    string = re.sub(r'http\S+', '', string)
    string = re.sub('@[^\s]+', '', string)
    string = re.sub(r'#[^\s]+', '', string)
    string = string.encode("ascii", "ignore")
    string = string.decode()
    string = re.sub('\s+',' ', string)
    return string

sentiment = ['negative', 'neutral', 'positive']

text = [cleansing(input_text)]
predicted = tokenizer.texts_to_sequences(text)
guess = pad_sequences(predicted, maxlen=X.shape[1])

model = load_model('model.h5')
prediction = model.predict(guess)
polarity = np.argmax(prediction[0])

print("Text: ",text[0])
print("Sentiment: ",sentiment[polarity])

